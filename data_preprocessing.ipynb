{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import preprocess_text\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ì›ë¬¸</th>\n",
       "      <th>ë²ˆì—­ë¬¸</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>749730</th>\n",
       "      <td>ê·¸ ë²¤ì¹˜ì—ì„œ ë‘ ì‚¬ëŒì´ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  íŠ¹íˆ ì¡°ìš©í•œ ë…ëŒ€ë¥¼ í•˜ë©´ì„œ ë§ˆìŒì„ ë‹¤ ì—´ê³  ...</td>\n",
       "      <td>It was noted that the two of them talked on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728687</th>\n",
       "      <td>ì§€ì—­ ê±°ì£¼í™˜ìê°€ ìì‹ ì˜ ì§€ì—­ ì˜ë£Œê¸°ê´€ì„ ì´ìš©í•˜ëŠ” ì •ë„ë¥¼ ì˜ë¯¸í•˜ëŠ” â€˜ì˜ë£Œì„œë¹„ìŠ¤ ì´ìš©ë¥ ...</td>\n",
       "      <td>The \"Medical Service Utilization Rate,\" which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684982</th>\n",
       "      <td>ì „ì„ ê¹€ì˜ê¸° ì´ì¬ê°€ ë°€ì–´ë¶™ì¸ ì‹ ì¥ ì œí•œ ê·œì •ì€ í•´ì™¸ í† í”½ìœ¼ë¡œ ì†Œê°œë  ë§Œí¼ êµ­ë‚´ì™¸ì ìœ¼...</td>\n",
       "      <td>The height restrictions imposed by the former ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120168</th>\n",
       "      <td>ì§€ë‚œí•´ëŠ” ì„±ë‚¨ê³¼ ìš©ì¸ì‹œì˜ ìì²´ì‚¬ì—…ìœ¼ë¡œ ì¤‘í•™êµ ì‹ ì…ìƒì— ëŒ€í•œ êµë³µì§€ì›ì´ ì´ë£¨ì–´ì§€ë©° êµ...</td>\n",
       "      <td>Last year, Seongnam-si and Yongin-si's own pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511621</th>\n",
       "      <td>ì–‘ì£¼ì‹œì˜íšŒì™€ ë™ë‘ì²œì–‘ì£¼êµìœ¡ì§€ì›ì²­ì´ ê³µë™ ì£¼ìµœí•˜ê³ , ê°ë™êµìœ¡ì„ ì‹¤ì²œí•˜ëŠ” 100ì¸ ì—°ëŒ€...</td>\n",
       "      <td>The Yangju Hope Education Forum, co-hosted by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       ì›ë¬¸  \\\n",
       "749730  ê·¸ ë²¤ì¹˜ì—ì„œ ë‘ ì‚¬ëŒì´ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  íŠ¹íˆ ì¡°ìš©í•œ ë…ëŒ€ë¥¼ í•˜ë©´ì„œ ë§ˆìŒì„ ë‹¤ ì—´ê³  ...   \n",
       "728687  ì§€ì—­ ê±°ì£¼í™˜ìê°€ ìì‹ ì˜ ì§€ì—­ ì˜ë£Œê¸°ê´€ì„ ì´ìš©í•˜ëŠ” ì •ë„ë¥¼ ì˜ë¯¸í•˜ëŠ” â€˜ì˜ë£Œì„œë¹„ìŠ¤ ì´ìš©ë¥ ...   \n",
       "684982  ì „ì„ ê¹€ì˜ê¸° ì´ì¬ê°€ ë°€ì–´ë¶™ì¸ ì‹ ì¥ ì œí•œ ê·œì •ì€ í•´ì™¸ í† í”½ìœ¼ë¡œ ì†Œê°œë  ë§Œí¼ êµ­ë‚´ì™¸ì ìœ¼...   \n",
       "120168  ì§€ë‚œí•´ëŠ” ì„±ë‚¨ê³¼ ìš©ì¸ì‹œì˜ ìì²´ì‚¬ì—…ìœ¼ë¡œ ì¤‘í•™êµ ì‹ ì…ìƒì— ëŒ€í•œ êµë³µì§€ì›ì´ ì´ë£¨ì–´ì§€ë©° êµ...   \n",
       "511621  ì–‘ì£¼ì‹œì˜íšŒì™€ ë™ë‘ì²œì–‘ì£¼êµìœ¡ì§€ì›ì²­ì´ ê³µë™ ì£¼ìµœí•˜ê³ , ê°ë™êµìœ¡ì„ ì‹¤ì²œí•˜ëŠ” 100ì¸ ì—°ëŒ€...   \n",
       "\n",
       "                                                      ë²ˆì—­ë¬¸  \n",
       "749730  It was noted that the two of them talked on th...  \n",
       "728687  The \"Medical Service Utilization Rate,\" which ...  \n",
       "684982  The height restrictions imposed by the former ...  \n",
       "120168  Last year, Seongnam-si and Yongin-si's own pro...  \n",
       "511621  The Yangju Hope Education Forum, co-hosted by ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data/aihub_news_data.csv\"\n",
    "\n",
    "df = pd.read_csv(data_dir)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801387\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Korean Text: ê²½ì´ì€ â€œì„ìœ í™”í•™ì´ë‚˜ ì² ê°•ì—…ì˜ ëŒ€ì •ë¹„, ì¡°ì„ ì—…ì˜ ì‹œìš´ì „, ê±´ì„¤ì—…ì˜ ê¸°ìƒì•…í™”ë¡œ ì¸í•œ ê³µê¸° ì§€ì—°, ë°©ì†¡Â·ì˜í™” ì œì‘ì—…ì˜ ì¸ë ¥ ëŒ€ì²´ ë¶ˆê°€ëŠ¥ ë“±ì´ ì¸ê°€ì—°ì¥ê·¼ë¡œê°€ í•„ìš”í•œ ëŒ€í‘œì ì¸ ì‚¬ë¡€â€ë¼ê³  ì£¼ì¥í–ˆë‹¤.\n",
      "Processed Korean Text: ê²½ì´ì€ ì„ìœ í™”í•™ì´ë‚˜ ì² ê°•ì—…ì˜ ëŒ€ì •ë¹„ ì¡°ì„ ì—…ì˜ ì‹œìš´ì „ ê±´ì„¤ì—…ì˜ ê¸°ìƒì•…í™”ë¡œ ì¸í•œ ê³µê¸° ì§€ì—° ë°©ì†¡ ì˜í™” ì œì‘ì—…ì˜ ì¸ë ¥ ëŒ€ì²´ ë¶ˆê°€ëŠ¥ ë“±ì´ ì¸ê°€ì—°ì¥ê·¼ë¡œê°€ í•„ìš”í•œ ëŒ€í‘œì ì¸ ì‚¬ë¡€ ë¼ê³  ì£¼ì¥í–ˆë‹¤.\n",
      "Original English Text: The Korea Enterprises Federation claimed that such cases as large maintenance of the petrochemical or steel industry, the trial voyage of the shipbuilding industry, schedule delay caused by bad weather in the construction industry, and the inability to replace manpower in broadcasting and film production industries are the representative examples that require an extended working hour.\n",
      "Processed English Text: The Korea Enterprises Federation claimed that such cases as large maintenance of the petrochemical or steel industry the trial voyage of the shipbuilding industry schedule delay caused by bad weather in the construction industry and the inability to replace manpower in broadcasting and film production industries are the representative examples that require an extended working hour.\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(df) - 1)\n",
    "\n",
    "processed_ko_text = preprocess_text(df[\"ì›ë¬¸\"][idx], lang=\"ko\")\n",
    "processed_en_text = preprocess_text(df[\"ë²ˆì—­ë¬¸\"][idx], lang=\"en\")\n",
    "\n",
    "print(\"Original Korean Text:\", df[\"ì›ë¬¸\"][idx])\n",
    "print(\"Processed Korean Text:\", processed_ko_text)\n",
    "print(\"Original English Text:\", df[\"ë²ˆì—­ë¬¸\"][idx])\n",
    "print(\"Processed English Text:\", processed_en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "ko_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "en_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™”ëœ í…ìŠ¤íŠ¸: ['ê²½ì´', '##ì€', 'ì„ìœ ', '##í™”í•™', '##ì´ë‚˜', 'ì² ê°•ì—…', '##ì˜', 'ëŒ€', '##ì •', '##ë¹„', 'ì¡°ì„ ì—…', '##ì˜', 'ì‹œ', '##ìš´ì „', 'ê±´ì„¤ì—…', '##ì˜', 'ê¸°ìƒ', '##ì•…', '##í™”', '##ë¡œ', 'ì¸í•œ', 'ê³µê¸°', 'ì§€ì—°', 'ë°©ì†¡', 'ì˜í™”', 'ì œì‘', '##ì—…', '##ì˜', 'ì¸ë ¥', 'ëŒ€ì²´', 'ë¶ˆê°€', '##ëŠ¥', 'ë“±', '##ì´', 'ì¸ê°€', '##ì—°', '##ì¥', '##ê·¼', '##ë¡œ', '##ê°€', 'í•„ìš”', '##í•œ', 'ëŒ€í‘œ', '##ì ì¸', 'ì‚¬ë¡€', 'ë¼ê³ ', 'ì£¼ì¥', '##í–ˆ', '##ë‹¤', '.']\n",
      "ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ëœ í† í°: [26479, 2073, 5728, 22563, 15351, 23719, 2079, 823, 2287, 2151, 15018, 2079, 1325, 10926, 12795, 2079, 7179, 2376, 2267, 2200, 5198, 5495, 7035, 3861, 3771, 4271, 2078, 2079, 4862, 4761, 5391, 2183, 886, 2052, 3955, 2156, 2121, 2169, 2200, 2116, 3677, 2470, 3661, 31221, 4411, 3609, 3831, 2371, 2062, 18]\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ë¥¼ í† í°í™”\n",
    "ko_tokens = ko_tokenizer.tokenize(processed_ko_text)\n",
    "print(\"í† í°í™”ëœ í…ìŠ¤íŠ¸:\", ko_tokens)\n",
    "\n",
    "# í† í°ì„ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "ko_input_ids = ko_tokenizer.convert_tokens_to_ids(ko_tokens)\n",
    "print(\"ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ëœ í† í°:\", ko_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™”ëœ í…ìŠ¤íŠ¸: ['the', 'korea', 'enterprises', 'federation', 'claimed', 'that', 'such', 'cases', 'as', 'large', 'maintenance', 'of', 'the', 'pet', '##ro', '##chemical', 'or', 'steel', 'industry', 'the', 'trial', 'voyage', 'of', 'the', 'shipbuilding', 'industry', 'schedule', 'delay', 'caused', 'by', 'bad', 'weather', 'in', 'the', 'construction', 'industry', 'and', 'the', 'inability', 'to', 'replace', 'manpower', 'in', 'broadcasting', 'and', 'film', 'production', 'industries', 'are', 'the', 'representative', 'examples', 'that', 'require', 'an', 'extended', 'working', 'hour', '.']\n",
      "ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ëœ í† í°: [1996, 4420, 9926, 4657, 3555, 2008, 2107, 3572, 2004, 2312, 6032, 1997, 1996, 9004, 3217, 15869, 2030, 3886, 3068, 1996, 3979, 8774, 1997, 1996, 16802, 3068, 6134, 8536, 3303, 2011, 2919, 4633, 1999, 1996, 2810, 3068, 1998, 1996, 13720, 2000, 5672, 22039, 1999, 5062, 1998, 2143, 2537, 6088, 2024, 1996, 4387, 4973, 2008, 5478, 2019, 3668, 2551, 3178, 1012]\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ë¥¼ í† í°í™”\n",
    "en_tokens = en_tokenizer.tokenize(processed_en_text)\n",
    "print(\"í† í°í™”ëœ í…ìŠ¤íŠ¸:\", en_tokens)\n",
    "\n",
    "# í† í°ì„ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "en_input_ids = en_tokenizer.convert_tokens_to_ids(en_tokens)\n",
    "print(\"ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ëœ í† í°:\", en_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í•œêµ­ì–´ ë¬¸ì¥ ê¸¸ì´ í†µê³„ (í† í° ìˆ˜ ê¸°ì¤€)\n",
      "- í‰ê· : 34.17534474604654\n",
      "- ì¤‘ì•™ê°’: 35.0\n",
      "- í‘œì¤€í¸ì°¨: 11.193086731190547\n",
      "ğŸ“Š ì˜ì–´ ë¬¸ì¥ ê¸¸ì´ í†µê³„ (í† í° ìˆ˜ ê¸°ì¤€)\n",
      "- í‰ê· : 32.65063945384689\n",
      "- ì¤‘ì•™ê°’: 33.0\n",
      "- í‘œì¤€í¸ì°¨: 12.090316986627746\n",
      "\n",
      "ğŸ“Œ í•œêµ­ì–´ ë¬¸ì¥ ê¸¸ì´ ìƒìœ„ 5% ì„ê³„ê°’: 52.0\n",
      "\n",
      "ğŸ“Œ ì˜ì–´ ë¬¸ì¥ ê¸¸ì´ ìƒìœ„ 5% ì„ê³„ê°’: 53.0\n"
     ]
    }
   ],
   "source": [
    "# í•œêµ­ì–´ ë¬¸ì¥ ê¸¸ì´ ê³„ì‚° (ì „ì²˜ë¦¬ + í† í¬ë‚˜ì´ì € ê¸°ì¤€)\n",
    "ko_lengths = df['ì›ë¬¸'].apply(lambda x: len(ko_tokenizer.tokenize(preprocess_text(str(x), lang=\"ko\"))))\n",
    "\n",
    "# ì˜ì–´ ë¬¸ì¥ ê¸¸ì´ ê³„ì‚° (ì „ì²˜ë¦¬ + í† í¬ë‚˜ì´ì € ê¸°ì¤€)\n",
    "en_lengths = df['ë²ˆì—­ë¬¸'].apply(lambda x: len(en_tokenizer.tokenize(preprocess_text(str(x), lang=\"en\"))))\n",
    "\n",
    "# ì „ì²´ í†µê³„ ì¶œë ¥\n",
    "print(\"ğŸ“Š í•œêµ­ì–´ ë¬¸ì¥ ê¸¸ì´ í†µê³„ (í† í° ìˆ˜ ê¸°ì¤€)\")\n",
    "print(\"- í‰ê· :\", ko_lengths.mean())\n",
    "print(\"- ì¤‘ì•™ê°’:\", ko_lengths.median())\n",
    "print(\"- í‘œì¤€í¸ì°¨:\", ko_lengths.std())\n",
    "\n",
    "print(\"ğŸ“Š ì˜ì–´ ë¬¸ì¥ ê¸¸ì´ í†µê³„ (í† í° ìˆ˜ ê¸°ì¤€)\")\n",
    "print(\"- í‰ê· :\", en_lengths.mean())\n",
    "print(\"- ì¤‘ì•™ê°’:\", en_lengths.median())\n",
    "print(\"- í‘œì¤€í¸ì°¨:\", en_lengths.std())\n",
    "\n",
    "# ìƒìœ„ 5% ê¸°ì¤€ ì„ê³„ê°’\n",
    "ko_threshold = ko_lengths.quantile(0.95)\n",
    "en_threshold = en_lengths.quantile(0.95)\n",
    "\n",
    "# ìƒìœ„ 5%ì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œë“¤ì˜ ê¸¸ì´\n",
    "ko_top5 = ko_lengths[ko_lengths >= ko_threshold]\n",
    "en_top5 = en_lengths[en_lengths >= en_threshold]\n",
    "\n",
    "print(f\"\\nğŸ“Œ í•œêµ­ì–´ ë¬¸ì¥ ê¸¸ì´ ìƒìœ„ 5% ì„ê³„ê°’: {ko_threshold}\")\n",
    "print(f\"\\nğŸ“Œ ì˜ì–´ ë¬¸ì¥ ê¸¸ì´ ìƒìœ„ 5% ì„ê³„ê°’: {en_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š í•œêµ­ì–´ ë°ì´í„°ì…‹\n",
      "- ì „ì²´ í† í° ìˆ˜: 27387677\n",
      "- ê³ ìœ  í† í° ìˆ˜: 29933\n",
      "\n",
      "ğŸ“Š ì˜ì–´ ë°ì´í„°ì…‹\n",
      "- ì „ì²´ í† í° ìˆ˜: 26165798\n",
      "- ê³ ìœ  í† í° ìˆ˜: 26585\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_ko_tokens = []\n",
    "all_en_tokens = []\n",
    "\n",
    "# ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ê³  í† í°í™”í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
    "for text in df['ì›ë¬¸']:\n",
    "    processed = preprocess_text(str(text), lang=\"ko\")\n",
    "    all_ko_tokens.extend(ko_tokenizer.tokenize(processed))\n",
    "\n",
    "for text in df['ë²ˆì—­ë¬¸']:\n",
    "    processed = preprocess_text(str(text), lang=\"en\")\n",
    "    all_en_tokens.extend(en_tokenizer.tokenize(processed))\n",
    "\n",
    "# ì „ì²´ í† í° ìˆ˜\n",
    "total_ko_tokens = len(all_ko_tokens)\n",
    "total_en_tokens = len(all_en_tokens)\n",
    "\n",
    "# ê³ ìœ  í† í° ìˆ˜\n",
    "unique_ko_tokens = len(set(all_ko_tokens))\n",
    "unique_en_tokens = len(set(all_en_tokens))\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ğŸ“Š í•œêµ­ì–´ ë°ì´í„°ì…‹\")\n",
    "print(\"- ì „ì²´ í† í° ìˆ˜:\", total_ko_tokens)\n",
    "print(\"- ê³ ìœ  í† í° ìˆ˜:\", unique_ko_tokens)\n",
    "\n",
    "print(\"\\nğŸ“Š ì˜ì–´ ë°ì´í„°ì…‹\")\n",
    "print(\"- ì „ì²´ í† í° ìˆ˜:\", total_en_tokens)\n",
    "print(\"- ê³ ìœ  í† í° ìˆ˜:\", unique_en_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š KO í† í° í†µê³„\n",
      "- ì „ì²´ í† í° ìˆ˜ (ì¤‘ë³µ í¬í•¨): 27387677\n",
      "- ê³ ìœ  í† í° ìˆ˜: 29933\n",
      "- ìƒìœ„ 95% ë¹ˆë„ë¥¼ ì»¤ë²„í•˜ëŠ” í† í° ìˆ˜: 9144\n",
      "ğŸ“Š EN í† í° í†µê³„\n",
      "- ì „ì²´ í† í° ìˆ˜ (ì¤‘ë³µ í¬í•¨): 26165798\n",
      "- ê³ ìœ  í† í° ìˆ˜: 26585\n",
      "- ìƒìœ„ 95% ë¹ˆë„ë¥¼ ì»¤ë²„í•˜ëŠ” í† í° ìˆ˜: 6522\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_vocab_cutoff(tokens, coverage=0.95, lang='ko'):\n",
    "    counter = Counter(tokens)\n",
    "    total_freq = sum(counter.values())\n",
    "    sorted_tokens = counter.most_common()\n",
    "    \n",
    "    cumulative = 0\n",
    "    cutoff = 0\n",
    "    \n",
    "    for i, (_, freq) in enumerate(sorted_tokens):\n",
    "        cumulative += freq\n",
    "        if cumulative / total_freq >= coverage:\n",
    "            cutoff = i + 1  # indexê°€ 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1\n",
    "            break\n",
    "    \n",
    "    print(f\"ğŸ“Š {lang.upper()} í† í° í†µê³„\")\n",
    "    print(f\"- ì „ì²´ í† í° ìˆ˜ (ì¤‘ë³µ í¬í•¨): {total_freq}\")\n",
    "    print(f\"- ê³ ìœ  í† í° ìˆ˜: {len(counter)}\")\n",
    "    print(f\"- ìƒìœ„ {coverage*100:.0f}% ë¹ˆë„ë¥¼ ì»¤ë²„í•˜ëŠ” í† í° ìˆ˜: {cutoff}\")\n",
    "    \n",
    "    return cutoff\n",
    "\n",
    "# ì „ì²˜ë¦¬ ë° í† í¬ë‚˜ì´ì¦ˆ\n",
    "all_ko_tokens = []\n",
    "all_en_tokens = []\n",
    "\n",
    "for text in df['ì›ë¬¸']:\n",
    "    processed = preprocess_text(str(text), lang=\"ko\")\n",
    "    all_ko_tokens.extend(ko_tokenizer.tokenize(processed))\n",
    "\n",
    "for text in df['ë²ˆì—­ë¬¸']:\n",
    "    processed = preprocess_text(str(text), lang=\"en\")\n",
    "    all_en_tokens.extend(en_tokenizer.tokenize(processed))\n",
    "\n",
    "# ìƒìœ„ 95% ì»¤ë²„ë¦¬ì§€ vocab size ê³„ì‚°\n",
    "ko_vocab_cutoff = compute_vocab_cutoff(all_ko_tokens, coverage=0.95, lang=\"ko\")\n",
    "en_vocab_cutoff = compute_vocab_cutoff(all_en_tokens, coverage=0.95, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
