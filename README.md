## Roadmap

### **프로젝트 목표**

- **신경망 기계 번역(영어 → 한국어)** 구현 with PyTorch
- Going Deeper NLP 과정에서 배운 전 과정을 복습

### **프로젝트 마일스톤**

1. **데이터 준비 및 전처리**
    - **데이터셋 수집**: 기계번역을 위한 한국어-영어 병렬 텍스트 데이터셋을 수집하고, 모델 학습에 적합한 형태로 준비   
    - **EDA & 텍스트 전처리**
      - EDA를 통해 적절한 데이터 전처리 방향을 결정
      - 데이터 정제 (소문자화, 불용어 제거 등) 및 텍스트 정규화 작업
      - 각 문장을 토큰화하여 단어 또는 subword 단위로 분할
        
    **⇒ 결과물**: 전처리를 거친 데이터셋
   
2. **모델 설계 및 구현**
   - **사전학습된 임베딩**
    - 사전 학습된 Word2vec 또는 FastText 임베딩을 사용하여 단어 벡터를 초기화합니다.
- **인코더-디코더 모델 설계**:
    - LSTM + Attention, Transformer 중 하나 선택
    - Pytorch로 모델 구현
- **BERT 계열 pre-trained model**:
    - Hugging Face의 BERT 계열 사전학습 모델을 인코더로 활용

    **⇒ 결과물**: 모델 아키텍처 설계 및 초기 구현 완료

3. **훈련 및 번역 문장 생성**
    - **훈련**: Pytorch로 모델 훈련을 위한 함수 구현 → 모델 학습
    - **문장 생성**: **Beam Search** 또는 **Sampling** 기법을 사용하여 모델의 추론 품질 향상
    - **성능 평가**: **BLEU score**를 사용하여 생성된 문장의 품질을 평가하고, 모델의 기계번역 성능을 측정
    
    **⇒ 결과물**: 모델 훈련 완료, 문장 생성 및 성능 평가 완료, BLEU score 결과
    
4. **모델 최적화 및 하이퍼파라미터 튜닝**
    - 하이퍼파라미터 튜닝을 통해 성능을 개선
    - 다양한 모델 실험을 통해 최적의 아키텍처와 파라미터 조합 찾기
    
    **⇒ 결과물**: 하이퍼파라미터 최적화 완료, 성능 개선
    
5. **결과 분석 및 보고서 작성**
    - 최종 성능 분석 및 결과 정리
        - BLEU score, PPL
        - 정성 평가
    - 모델 학습 과정과 결과를 바탕으로 회고 작성
    - Further research : 기계번역 모델의 성능을 향상시키기 위한 테크닉 추가
        - input feeding
        - copied translation을 통한 데이터 증강
        - etc
    
    **⇒ 결과물**: 최종 회고
